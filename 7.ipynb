{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Exercise 7 - Evaluation\n",
    "\n",
    "Issued: May 27, 2025\n",
    "\n",
    "Due: June 2, 2025\n",
    "\n",
    "Please submit this filled sheet via Ilias by the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Metric Calibration\n",
    "Search evaluation scores are often bounded in the `[0,1]` interval to make them comparable. This range (wrongly!) suggests that these scores can be interpreted as probabilities. \n",
    "\n",
    "a) Implement the computation of average precision and compute AP scores for three example systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.16666666666666666\n",
      "0.22624338624338622\n"
     ]
    }
   ],
   "source": [
    "#The following three rankings have been produced by three independent search systems. Each entry refers to the rank of the result, the document ID, the retrieval model score and whether or not the document was judged relevant to this query.\n",
    "S1 = [\n",
    "    [1,  'D24', 0.99, 1],\n",
    "    [2,  'D86', 0.98, 1],\n",
    "    [3,  'D74', 0.95, 1],\n",
    "    [4,  'D95', 0.95, 1],\n",
    "    [5,  'D11', 0.93, 1],\n",
    "    [6,  'D49', 0.89, 0],\n",
    "    [7,  'D67', 0.89, 0],\n",
    "    [8,  'D54', 0.87, 0],\n",
    "    [9,  'D37', 0.83, 0],\n",
    "    [10, 'D30', 0.81, 0]\n",
    "]\n",
    "\n",
    "S2 = [\n",
    "    [1,  'D83', 0.99, 0],\n",
    "    [2,  'D51', 0.81, 1],\n",
    "    [3,  'D44', 0.80, 0],\n",
    "    [4,  'D65', 0.75, 1],\n",
    "    [5,  'D32', 0.71, 0],\n",
    "    [6,  'D16', 0.68, 1],\n",
    "    [7,  'D87', 0.63, 0],\n",
    "    [8,  'D12', 0.21, 1],\n",
    "    [9,  'D59', 0.20, 0],\n",
    "    [10, 'D24', 0.18, 1]\n",
    "]\n",
    "\n",
    "S3 = [\n",
    "    [1,  'D14', 0.89, 1],\n",
    "    [2,  'D44', 0.81, 0],\n",
    "    [3,  'D35', 0.81, 1],\n",
    "    [4,  'D41', 0.72, 0],\n",
    "    [5,  'D51', 0.71, 1],\n",
    "    [6,  'D55', 0.67, 0],\n",
    "    [7,  'D63', 0.63, 1],\n",
    "    [8,  'D70', 0.60, 0],\n",
    "    [9,  'D11', 0.57, 1],\n",
    "    [10, 'D90', 0.55, 0]\n",
    "]\n",
    "\n",
    "#Compute Average Precision based on a ranked result list and the overall number of relevant documents\n",
    "def ap(ranking, R):\n",
    "    rel = 0\n",
    "    prec = []\n",
    "    for d in ranking:\n",
    "        if d[3]:\n",
    "            rel += 1\n",
    "            prec.append(rel / d[0])\n",
    "    return sum(prec) / R if R > 0 else 0\n",
    "\n",
    "for system in [S1, S2, S3]:\n",
    "    print(ap(system, 15))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now imagine a fourth system with the following score distribution. How should it be ranked relative to the original three systems?\n",
    "\n",
    "```\n",
    "S4 = [\n",
    "    [1,  'D24', 0.90, 1],\n",
    "    [2,  'D86', 0.80, 1],\n",
    "    [3,  'D74', 0.85, 1],\n",
    "    [4,  'D95', 0.75, 1],\n",
    "    [5,  'D11', 0.68, 1],\n",
    "    [6,  'D49', 0.62, 0],\n",
    "    [7,  'D67', 0.55, 0],\n",
    "    [8,  'D54', 0.40, 0],\n",
    "    [9,  'D37', 0.34, 0],\n",
    "    [10, 'D30', 0.27, 0]\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be ranked exactly the same as S1 since the AP only depends on column 4 of the data (whether a document is relevant) and this is the same in S1 and S4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query Performance Prediction\n",
    "\n",
    "Imagine you had many pairs of queries Q and nDCG scores s for a retrieval system. Would you be able to skip the retrieval model and performance metric calculation and directly predict how well the system would do for a given query? Why does this (not) work? Discuss all pertinent considerations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For queries existing inside the pairs we have nDCG scores so predicting how well the model perform is possible. In the general case it depends on how well the queries cover queries which could be asked, if we cover more queries and especially vastly different queries we can predict more of the generalization capabilities. Also if we can create similarity scores from new queries to existing queries (in our pairs) we could also use this to say how well we can predict the performance of the system, being able to predict with higher accuracy for queries more similar to existing queries in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Project Team Registration\n",
    "\n",
    "By the same due date as this homework (June 2nd), please also fill the project team registration form: https://forms.gle/KiLZS4A4T3ckswXS6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
