{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Exercise 1 - Web Crawling\n",
    "\n",
    "Issued: April 16, 2024\n",
    "\n",
    "Due: April 22, 2024\n",
    "\n",
    "Please submit this filled sheet via Ilias by the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Duplicate Detection\n",
    "When crawling large numbers of Web pages we are likely to encounter a considerable number of duplicate documents. To not flood our index with replicas of the same documents, we need a duplicate detection scheme.\n",
    "\n",
    "a) Using python's built-in hash() function, process the following documents in order of appearance and flag up any exact duplicates.\n",
    "\n",
    "- **D1** \"This is just some document\"\n",
    "- **D2** \"This is another piece of text\"\n",
    "- **D3** \"This is another piece of text\"\n",
    "- **D4** \"This is just some documents\"\n",
    "- **D5** \"Totally different stuff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8972256982357890143"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(\"This is just some document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check a single document against an existing collection of previsouly seen documents for exact duplicates.\n",
    "def check_exct(doc, docs):\n",
    "    #TODO: Implement exact duplicate detection\n",
    "    return hash(doc) in [hash(d) for d in docs] #TODO: Return True if the document is a duplicate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Going beyond exact duplicates, we want to also identify any near-duplicates that are very similar but not identical to previously seen content. Implement the SimHash method discussed in class and again process the five documents, this time flagging up exact and near duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "#Check a single document against an existing collection of previsouly seen documents for near duplicates\n",
    "def check_simhash(doc, docs):\n",
    "    #TODO: Implement near duplicate detection\n",
    "    def rhash(snt):\n",
    "        snt = snt.lower()\n",
    "        splt = snt.split(\" \")\n",
    "        counts = Counter(splt)\n",
    "        words = list(counts.keys())\n",
    "        vals = [0] * sys.hash_info.width\n",
    "        for word in words:\n",
    "            lst = list(bin(abs(hash(word)))[2:])\n",
    "            lst = [int(i) for i in lst]\n",
    "            lst = [1 if i > 0 else -1 for i in lst]\n",
    "            vals = [lv * counts[word] + v for lv, v in zip(lst, vals)]\n",
    "        return int(''.join([str(1) if v > 0 else str(0) for v in vals]), 2)\n",
    "\n",
    "    doc_hash = rhash(doc)\n",
    "    for d in docs:\n",
    "        sim = (doc_hash ^ rhash(d)).bit_count()\n",
    "        print(\"Difference\", sim)\n",
    "        if sim < 10:\n",
    "            return True\n",
    "    return False #TODO: Return True if the document is a duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference 24\n",
      "Difference 24\n",
      "Difference 0\n",
      "DUPLICATE: D3\n",
      "Difference 9\n",
      "DUPLICATE: D4\n",
      "Difference 28\n",
      "Difference 30\n"
     ]
    }
   ],
   "source": [
    "crawl = [['D1', 'This is just some document'], ['D2', 'This is another piece of text'], ['D3', 'This is another piece of text'], ['D4', 'This is just some documents'], ['D5', 'Totally different stuff']]\n",
    "\n",
    "#Process raw crawled website content\n",
    "def process(crawl):\n",
    "    docs = []\n",
    "    for doc in crawl:\n",
    "        if check_simhash(doc[1], [y for _, y in docs]): #Can be exchanged for check_simhash()\n",
    "            print('DUPLICATE: '+doc[0])\n",
    "        else:\n",
    "            docs.append(doc)\n",
    "\n",
    "process(crawl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Focused Search Engines\n",
    "Suppose you were to build a COVID-19 Web search engine for which you want to collect and eventually serve only COVID-19 information. The general web crawling process follows this scheme:\n",
    "\n",
    "1. Create a seed set of known URLs (a.k.a the frontier)\n",
    "2. Pull a URL from the frontier and visit it\n",
    "3. Save the page content for our search engine (indexing)\n",
    "4. Once on the page, note down all URLs linked there\n",
    "5. Put all encountered URLs in the queue\n",
    "6. Repeat from Step 2 until the queue is empty\n",
    "\n",
    "In this particular setting, how should the generic step-by-step crawling process be modified/extended? Discuss all relevant considerations:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using medical and news pages as the frontier should get most important information. It may be possible to avoid unrelated pages but is very difficult as most pages could contain information on COVID-19 in some form.\n",
    "- To avoid unnecessary crawling you could rank pages on their relevance related to COVID and if they rank below a certain threshold their URLs are not added to the frontier. But this could lead to the crawler missing out/not finding interesting pages hidden behind boring pages.\n",
    "- The frontier queue could be ranked by the context of the links where links with medical context like from a post about COVID are ranked higher than random links without additional information. This could also be used to avoid unhelpful links (a post about gaming will rarely link to information about COVID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
