{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Exercise 2 - Text Representation\n",
    "\n",
    "Issued: April 22, 2025\n",
    "\n",
    "Due: April 28, 2025\n",
    "\n",
    "Please submit this filled sheet via Ilias by the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bag-of-Words Models\n",
    "In class we discussed BOW vectorization models under which documents are represented via term frequency counts.\n",
    "\n",
    "a) Construct term frequency BOW representations for the following sentences:\n",
    "\n",
    "- \"The government is open.\"\n",
    "- \"The government is closed.\"\n",
    "- \"Long live Mickey Mouse, emperor of all!\"\n",
    "- \"Darn! This will break.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'of': 0,\n",
       "  'mouse': 0,\n",
       "  'government': 1,\n",
       "  'closed': 0,\n",
       "  'emperor': 0,\n",
       "  'will': 0,\n",
       "  'long': 0,\n",
       "  'the': 1,\n",
       "  'mickey': 0,\n",
       "  'all': 0,\n",
       "  'this': 0,\n",
       "  'live': 0,\n",
       "  'is': 1,\n",
       "  'break': 0,\n",
       "  'darn': 0,\n",
       "  'open': 1},\n",
       " {'of': 0,\n",
       "  'mouse': 0,\n",
       "  'government': 1,\n",
       "  'closed': 1,\n",
       "  'emperor': 0,\n",
       "  'will': 0,\n",
       "  'long': 0,\n",
       "  'the': 1,\n",
       "  'mickey': 0,\n",
       "  'all': 0,\n",
       "  'this': 0,\n",
       "  'live': 0,\n",
       "  'is': 1,\n",
       "  'break': 0,\n",
       "  'darn': 0,\n",
       "  'open': 0},\n",
       " {'of': 1,\n",
       "  'mouse': 1,\n",
       "  'government': 0,\n",
       "  'closed': 0,\n",
       "  'emperor': 1,\n",
       "  'will': 0,\n",
       "  'long': 1,\n",
       "  'the': 0,\n",
       "  'mickey': 1,\n",
       "  'all': 1,\n",
       "  'this': 0,\n",
       "  'live': 1,\n",
       "  'is': 0,\n",
       "  'break': 0,\n",
       "  'darn': 0,\n",
       "  'open': 0},\n",
       " {'of': 0,\n",
       "  'mouse': 0,\n",
       "  'government': 0,\n",
       "  'closed': 0,\n",
       "  'emperor': 0,\n",
       "  'will': 1,\n",
       "  'long': 0,\n",
       "  'the': 0,\n",
       "  'mickey': 0,\n",
       "  'all': 0,\n",
       "  'this': 1,\n",
       "  'live': 0,\n",
       "  'is': 0,\n",
       "  'break': 1,\n",
       "  'darn': 1,\n",
       "  'open': 0}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "corpus = [['The government is open.'], ['The government is closed.'], ['Long live Mickey Mouse, emperor of all!'], ['Darn! This will break.']]\n",
    "\n",
    "#Turn a corpus of arbitrary texts into term-frequency weighted BOW vectors.\n",
    "def TF(corpus):\n",
    "    vecs = []\n",
    "    wordlists = []\n",
    "    for doc in corpus:\n",
    "        vec = {}\n",
    "        sntc = doc[0].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        wordlists.append(sntc.split())\n",
    "    lex = set([word for words in wordlists for word in words])\n",
    "\n",
    "    for words in wordlists:\n",
    "        vec = {key: 0 for key in lex}\n",
    "        for word in words:\n",
    "            if word not in vec:\n",
    "                vec[word] = 1\n",
    "            else:\n",
    "                vec[word] += 1\n",
    "        vecs.append(vec)\n",
    "        # print(vec)\n",
    "    return vecs\n",
    "\n",
    "TF(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Extend the term frequency model by an inverse document frequency (IDF) component. Estimate IDFs based on the Reuters 21578 collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7769 total train documents\n",
      "The first document contains 633 words.\n",
      "Here they are:\n",
      "['BAHIA', 'COCOA', 'REVIEW', 'Showers', 'continued', ...]\n",
      "Processing IDF complete. unique words: 26410 documents: 7769\n",
      "[('insects', 8.95789673495042), ('bettered', 8.95789673495042), ('plus', 4.393548543482584), ('pomona', 8.95789673495042), ('rested', 8.264749554390475)]\n",
      "Processing IDF complete. unique words: 26410 documents: 7769\n",
      "Generating TFIDF completed. Generated 7769 vectors.\n",
      "tfidf: ('comissaria', 44.789483674752105)  idf : 8.95789673495042\n",
      "tfidf: ('bahia', 37.858011869152655)  idf : 7.5716023738305305\n",
      "tfidf: ('cocoa', 34.162515037312915)  idf : 4.880359291044702\n",
      "tfidf: ('times', 31.206609452341088)  idf : 4.458087064620155\n",
      "tfidf: ('sept', 27.619547652326375)  idf : 5.523909530465275\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "#Download the documents\n",
    "nltk.download(\"reuters\")\n",
    "documents = reuters.fileids()\n",
    "\n",
    "docs = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "print(str(len(docs)) + \" total train documents\")\n",
    "\n",
    "#To access the content of a news article, we can use the reuters.words() function\n",
    "print(\"The first document contains \"+str(len(reuters.words(docs[0])))+\" words.\\nHere they are:\")\n",
    "# for word in reuters.words(docs[0]): print(word)\n",
    "print(reuters.words(docs[0]))\n",
    "\n",
    "#Estimate inverse document frequencies based on a corpus of documents.\n",
    "def IDF(corpus):\n",
    "    idfs = {}\n",
    "    filtered = []\n",
    "    for doc in corpus:\n",
    "        f = reuters.words(doc)\n",
    "        f = [word.lower() for word in f if word not in string.punctuation]\n",
    "        filtered.extend(set(f))\n",
    "\n",
    "    counts = Counter(filtered)\n",
    "    for word in set(filtered):\n",
    "        idfs[word] = math.log(len(corpus) / counts[word])\n",
    "    print(\"Processing IDF complete. unique words:\", len(idfs), \"documents:\", len(corpus))\n",
    "    return idfs\n",
    "\n",
    "#Turn a corpus of arbitrary texts into TF-IDF weighted BOW vectors.\n",
    "def TFIDF(corpus):\n",
    "    vecs = []\n",
    "    idf = IDF(corpus)\n",
    "    filtered = []\n",
    "    for doc in corpus:\n",
    "        f = reuters.words(doc)\n",
    "        f = [word.lower() for word in f if word not in string.punctuation]\n",
    "        filtered.append(f)\n",
    "\n",
    "    vec_base = {key: 0 for key in idf.keys()}\n",
    "    for i, doc in enumerate(filtered):\n",
    "        vec = vec_base.copy()\n",
    "        counts = Counter(doc)\n",
    "        for word in set(doc):\n",
    "            vec[word] = counts[word] * idf[word]\n",
    "        vecs.append(vec)\n",
    "        if i % 1000 == 0: print(i, \"of\", len(corpus), \"documents processed.\", end=\"\\r\")\n",
    "    print(\"Generating TFIDF completed. Generated\", len(vecs), \"vectors.\")\n",
    "    return vecs\n",
    "\n",
    "idf = IDF(docs)\n",
    "print(list(idf.items())[:5])\n",
    "tfidf = TFIDF(docs)\n",
    "sorted_items = sorted(tfidf[0].items(), key=lambda kv: kv[1], reverse=True)\n",
    "for i in range(5):\n",
    "    print(f\"tfidf: {sorted_items[i]}\", f\" idf : {idf[sorted_items[i][0]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Bag-of-words models are order invariant. They do not retain the ordering in which terms occur in the document. Is there any way to include term order information in these models? Justify your answer below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retaining order could be somewhat possible when saving the vectors in dicts. This would make it possible to track which words occur first before other words by changing the order of the vector values in the dict."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Models\n",
    "Topic models represent textual documents in terms of their distribution of latent topics. Imagine you have trained a 10-topic LDA model. Each topic is a frequency distribution over thousands of terms. Is there a good way of illustrating the meaning of the learned topics to a human? Discuss the advantages and disadvantages of some of the possible options below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When stopwords are removed from the classification it could be very helpful to look at the most frequent words for each topic (e.g. word cloud). These words should have high importance for the generated categories and could give a good overview over the topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
