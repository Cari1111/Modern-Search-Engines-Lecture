{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Group Project\n",
    "\n",
    "Issued: June 17, 2025\n",
    "\n",
    "Due: July 21, 2025\n",
    "\n",
    "Please submit a link to your code repository (with a branch that does not change anymore after the submission deadline) and your 4-page report via email to carsten.eickhoff@uni-tuebingen.de by the due date. One submission per team.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Crawling & Indexing\n",
    "Crawl the web to discover **English content related to Tübingen**. The crawled content should be stored locally. If interrupted, your crawler should be able to re-start and pick up the crawling process at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot scrape\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import urllib.robotparser as urobot\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://google.com/\"\n",
    "rp = urobot.RobotFileParser()\n",
    "rp.set_url(url + \"robots.txt\")\n",
    "rp.read()\n",
    "if rp.can_fetch(\"*\", url + \"catalogs/about\"):\n",
    "    site = urllib.request.urlopen(url)\n",
    "    sauce = site.read()\n",
    "    soup = BeautifulSoup(sauce, \"html.parser\")\n",
    "    actual_url = site.geturl()[:site.geturl().rfind('/')]\n",
    "\n",
    "    my_list = soup.find_all(\"a\", href=True)\n",
    "    for i in my_list:\n",
    "        # rather than != \"#\" you can control your list before loop over it\n",
    "        if i != \"#\":\n",
    "            newurl = str(actual_url)+\"/\"+str(i)\n",
    "            try:\n",
    "                if rp.can_fetch(\"*\", newurl):\n",
    "                    site = urllib.request.urlopen(newurl)\n",
    "                    # do what you want on each authorized webpage\n",
    "            except:\n",
    "                pass\n",
    "else:\n",
    "    print(\"cannot scrape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 21:39:19,188 INFO: Cleaning up old proxies...\n",
      "2025-06-29 21:39:19,188 INFO: Fetching proxies list...\n",
      "2025-06-29 21:39:19,721 INFO: Found 4024 candidate proxies. Testing...\n",
      "2025-06-29 21:40:22,599 INFO: 24 proxies valid after testing.\n",
      "2025-06-29 21:40:32,793 INFO: Proxy http://138.68.60.8:8080 failed 1 time(s)\n",
      "2025-06-29 21:40:32,794 WARNING: Proxy http://138.68.60.8:8080 failed on attempt 1: HTTPSConnectionPool(host='duckduckgo.com', port=443): Read timed out. (read timeout=10)\n",
      "2025-06-29 21:40:42,973 INFO: Proxy http://138.68.60.8:8080 failed 2 time(s)\n",
      "2025-06-29 21:40:42,973 INFO: Dropping proxy http://138.68.60.8:8080 after 2 failures\n",
      "2025-06-29 21:40:42,974 WARNING: Proxy http://138.68.60.8:8080 failed on attempt 2: HTTPSConnectionPool(host='duckduckgo.com', port=443): Read timed out. (read timeout=10)\n",
      "2025-06-29 21:40:48,565 INFO: Proxy http://43.198.103.235:3128 failed 1 time(s)\n",
      "2025-06-29 21:40:48,565 WARNING: Proxy http://43.198.103.235:3128 failed on attempt 3: HTTPSConnectionPool(host='duckduckgo.com', port=443): Max retries exceeded with url: /html/?q=t%C3%BCbingen%20english (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 502 Bad Gateway')))\n",
      "2025-06-29 21:40:56,021 INFO: Proxy http://35.179.146.181:3128 failed 1 time(s)\n",
      "2025-06-29 21:40:56,021 WARNING: Proxy http://35.179.146.181:3128 failed on attempt 4: HTTPSConnectionPool(host='html.duckduckgo.com', port=443): Max retries exceeded with url: /html/?q=t%C3%BCbingen%20english (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 502 Bad Gateway')))\n",
      "2025-06-29 21:40:56,716 INFO: Proxy http://35.179.146.181:3128 failed 2 time(s)\n",
      "2025-06-29 21:40:56,716 INFO: Dropping proxy http://35.179.146.181:3128 after 2 failures\n",
      "2025-06-29 21:40:56,716 WARNING: Proxy http://35.179.146.181:3128 failed on attempt 5: HTTPSConnectionPool(host='html.duckduckgo.com', port=443): Max retries exceeded with url: /html/?q=t%C3%BCbingen%20english (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\n",
      "2025-06-29 21:41:06,769 INFO: Proxy http://161.35.70.249:8080 failed 1 time(s)\n",
      "2025-06-29 21:41:06,769 WARNING: Proxy http://161.35.70.249:8080 failed on attempt 6: HTTPSConnectionPool(host='duckduckgo.com', port=443): Read timed out. (read timeout=10)\n",
      "2025-06-29 21:41:16,898 INFO: Proxy http://209.97.150.167:8080 failed 1 time(s)\n",
      "2025-06-29 21:41:16,898 WARNING: Proxy http://209.97.150.167:8080 failed on attempt 7: HTTPSConnectionPool(host='duckduckgo.com', port=443): Read timed out. (read timeout=10)\n",
      "2025-06-29 21:41:27,022 INFO: Proxy http://209.97.150.167:8080 failed 2 time(s)\n",
      "2025-06-29 21:41:27,022 INFO: Dropping proxy http://209.97.150.167:8080 after 2 failures\n",
      "2025-06-29 21:41:27,023 WARNING: Proxy http://209.97.150.167:8080 failed on attempt 8: HTTPSConnectionPool(host='duckduckgo.com', port=443): Read timed out. (read timeout=10)\n",
      "2025-06-29 21:41:34,038 INFO: Visited 1 pages (Visited https://duckduckgo.com/html/?q=tübingen%20english, english=True, keywords_found=True)\n",
      "2025-06-29 21:41:35,040 INFO: Crawling complete.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import project\n",
    "\n",
    "start_urls = [r\"https://duckduckgo.com/html/?q=tübingen%20english\",\n",
    "            #   \"https://en.wikipedia.org/wiki/T%C3%BCbingen\",\n",
    "            #   'https://www.mygermanyvacation.com/best-things-to-do-and-see-in-tubingen-germany/',\n",
    "            #   'https://www.britannica.com/place/Tubingen-Germany',\n",
    "            #   'https://www.germany.travel/en/cities-culture/tuebingen.html',\n",
    "            #   'https://visit-tubingen.co.uk/',\n",
    "              ]\n",
    "crawler = project.Crawler(urls=start_urls, max_workers=30)\n",
    "crawler.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'https://duckduckgo.com/html/?q=tübingen%20english'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = {p for p, d in crawler.visited_pages.items() if d[\"keywords_found\"] and d[\"is_english\"]}\n",
    "print(len(filtered))\n",
    "filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query Processing \n",
    "Process a textual query and return the 100 most relevant documents from your index. Please incorporate **at least one retrieval model innovation** that goes beyond BM25 or TF-IDF. Please allow for queries to be entered either individually in an interactive user interface (see also #3 below), or via a batch file containing multiple queries at once. The batch file (see `queries.txt` for an example) will be formatted to have one query per line, listing the query number, and query text as tab-separated entries. An example of the batch file for the first two queries looks like this:\n",
    "\n",
    "```\n",
    "1   tübingen attractions\n",
    "2   food and drinks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve documents relevnt to a query. You need (at least) two parameters:\n",
    "\t#query: The user's search query\n",
    "\t#index: The location of the local index storing the discovered documents.\n",
    "def retrieve(query, index):\n",
    "    #TODO: Implement me\n",
    "\tpass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Search Result Presentation\n",
    "Once you have a result set, we want to return it to the searcher in two ways: a) in an interactive user interface. For this user interface, please think of **at least one innovation** that goes beyond the traditional 10-blue-links interface that most commercial search engines employ. b) as a text file used for batch performance evaluation. The text file should be formatted to produce one ranked result per line, listing the query number, rank position, document URL and relevance score as tab-separated entries. An example of the first three lines of such a text file looks like this:\n",
    "\n",
    "```\n",
    "1   1   https://www.tuebingen.de/en/3521.html   0.725\n",
    "1   2   https://www.komoot.com/guide/355570/castles-in-tuebingen-district   0.671\n",
    "1   3   https://www.unimuseum.uni-tuebingen.de/en/museum-at-hohentuebingen-castle   0.529\n",
    "...\n",
    "1   100 https://www.tuebingen.de/en/3536.html   0.178\n",
    "2   1   https://www.tuebingen.de/en/3773.html   0.956\n",
    "2   2   https://www.tuebingen.de/en/4456.html   0.797\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement an interactive user interface for part a of this exercise.\n",
    "\n",
    "#Produce a text file with 100 results per query in the format specified above.\n",
    "def batch(results):\n",
    "    #TODO: Implement me.    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performance Evaluation \n",
    "We will evaluate the performance of our search systems on the basis of five queries. Two of them are avilable to you now for engineering purposes:\n",
    "- `tübingen attractions`\n",
    "- `food and drinks`\n",
    "\n",
    "The remaining three queries will be given to you during our final session on July 22nd. Please be prepared to run your systems and produce a single result file for all five queries live in class. That means you should aim for processing times of no more than ~1 minute per query. We will ask you to send carsten.eickhoff@uni-tuebingen.de that file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "Your final projects will be graded along the following criteria:\n",
    "- 25% Code correctness and quality (to be delivered on this sheet)\n",
    "- 25% Report (4 pages, PDF, explanation and justification of your design choices)\n",
    "- 25% System performance (based on how well your system performs on the 5 queries relative to the other teams in terms of nDCG)\n",
    "- 15% Creativity and innovativeness of your approach (in particular with respect to your search system #2 and user interface #3 innovations)\n",
    "- 10% Presentation quality and clarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissible libraries\n",
    "You can use any general-puprose ML and NLP libraries such as scipy, numpy, scikit-learn, spacy, nltk, but please stay away from dedicated web crawling or search engine toolkits such as scrapy, whoosh, lucene, terrier, galago and the likes. Pretrained models are fine to use as part of your system, as long as they have not been built/trained for retrieval. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
